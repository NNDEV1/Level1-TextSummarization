# -*- coding: utf-8 -*-
"""T5TextSummarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-wq2xaSW-knmw7PXXv6PJ4zVpxpSF-U
"""

!nvidia-smi

!pip install transformers==4.5.0
!pip install pytorch.lightning==1.2.7

import json
import pandas as pd
import numpy as np
import torch
import pytorch_lightning as pl
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
from sklearn.model_selection import train_test_split
from termcolor import colored
import textwrap

from transformers import (T5ForConditionalGeneration,
                          AdamW)

from transformers import T5TokenizerFast as T5Tokenizer

from tqdm.auto import tqdm

!unzip "/content/archive (3).zip"

df = pd.read_csv("/content/news_summary.csv", encoding="latin-1")

df

df = pd.DataFrame(data=(df["text"], df["ctext"]))

df = df.T

df

df.columns = ["summary", "text"]

df = df.dropna()

df

train_df, test_df = train_test_split(df, test_size=0.15)

train_df.shape, test_df.shape

class SummaryDataset(Dataset):

    def __init__(self, 
                 data,
                 tokenizer,
                 text_max_tok_len=512,
                 summary_max_tok_len=128):
        
        self.tokenizer = tokenizer
        self.data = data
        self.text_max_tok_len = text_max_tok_len
        self.summary_max_tok_len = summary_max_tok_len

    def __len__(self):

        return len(self.data)

    def __getitem__(self, index):

        data_row = self.data.iloc[index]

        src_text = data_row["text"]

        text_encoding = self.tokenizer(
            src_text,
            max_length=self.text_max_tok_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )

        summary = data_row["summary"]

        summary_encoding = self.tokenizer(
            summary,
            max_length=self.summary_max_tok_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors="pt"
        )

        labels = summary_encoding["input_ids"]
        labels[labels == 0] = -100

        return dict(text=src_text,
                    summary=data_row["summary"],
                    text_input_ids=text_encoding["input_ids"].flatten(),
                    text_attention_mask=text_encoding["attention_mask"].flatten(),
                    labels=labels.flatten(),
                    labels_attention_mask=summary_encoding["attention_mask"].flatten())

class SummaryDataModule(pl.LightningDataModule):

    def __init__(self, 
                 train_df,
                 test_df,
                 tokenizer,
                 batch_size=8,
                 text_max_tok_len=512,
                 summary_max_tok_len=128):
        
        super().__init__()

        self.train_df = train_df
        self.test_df = test_df

        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.text_max_tok_len = text_max_tok_len
        self.summary_max_tok_len = summary_max_tok_len

    def setup(self, stage=None):

        self.train_dataset = SummaryDataset(self.train_df,
                                            self.tokenizer,
                                            self.text_max_tok_len,
                                            self.summary_max_tok_len)
        
        self.test_dataset = SummaryDataset(self.test_df,
                                            self.tokenizer,
                                            self.text_max_tok_len,
                                            self.summary_max_tok_len)
        
    def train_dataloader(self):

        return DataLoader(self.train_dataset,
                          batch_size=self.batch_size,
                          shuffle=True,
                          num_workers=2)
        
    def val_dataloader(self):

        return DataLoader(self.test_dataset,
                          batch_size=self.batch_size,
                          shuffle=False,
                          num_workers=2)
        
    def test_dataloader(self):

        return DataLoader(self.test_dataset,
                          batch_size=self.batch_size,
                          shuffle=False,
                          num_workers=2)

MODEL_NAME = "t5-base"

tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)


N_EPOCHS = 6
BATCH_SIZE = 8
data_module = SummaryDataModule(train_df, test_df, tokenizer, batch_size=BATCH_SIZE)

class SummaryModel(pl.LightningModule):

    def __init__(self):

        super(SummaryModel, self).__init__()

        self.summary_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):

        x = self.summary_model(input_ids,
                       attention_mask=attention_mask,
                       decoder_attention_mask=decoder_attention_mask,
                       labels=labels
                       )
        
        return x.logits, x.loss

    def step(self, batch, batch_idx, mode="train"):

        input_ids = batch["text_input_ids"]
        attention_mask = batch["text_attention_mask"]
        labels = batch["labels"]
        labels_attention_mask = batch["labels_attention_mask"]

        outputs, loss = self(input_ids, attention_mask, labels_attention_mask, labels)

        self.log(f"{mode}_loss", loss, prog_bar=True, logger=True)

        return loss

    def training_step(self, batch, batch_idx):

        return self.step(batch, batch_idx, mode="train")

    def validation_step(self, batch, batch_idx):

        return self.step(batch, batch_idx, mode="val")

    def test_step(self, batch, batch_idx):

        return self.step(batch, batch_idx, mode="test")

    def configure_optimizers(self):

        return AdamW(self.parameters(), lr=1e-4)

model = SummaryModel()

checkpoint_callback = ModelCheckpoint(
    dirpath="/content/checkpoints",
    filename="model-checkpoint",
    monitor="val_loss",
    verbose=True,
    save_top_k=1,
    mode="min"
)

trainer = pl.Trainer(checkpoint_callback=checkpoint_callback,
                     max_epochs=N_EPOCHS,
                     gpus=1,
                     progress_bar_refresh_rate=30)

trainer.fit(model, datamodule=data_module)

trained_model = SummaryModel.load_from_checkpoint(
    trainer.checkpoint_callback.best_model_path
)

trained_model.freeze()

def summarize(text):

    text_encoding = tokenizer(
        text,
        max_length=512,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        add_special_tokens=True,
        return_tensors="pt"
    )

    generated_ids = trained_model.summary_model.generate(
        input_ids=text_encoding["input_ids"],
        attention_mask=text_encoding["attention_mask"],
        max_length=128,
        num_beams=4,
        repetition_penalty=2.5,
        length_penalty=1.0,
        early_stopping=True
    )

    preds = [tokenizer.decode(gen_id, 
                              skip_special_tokens=True, 
                              clean_up_tokenization_spaces=True) for gen_id in generated_ids]

    return " ".join(preds)

test_df.iloc[33]["text"]

text = test_df.iloc[33]

real = text["summary"]
text = text["text"]

text

summary = summarize(text)

print(summary)

n99real